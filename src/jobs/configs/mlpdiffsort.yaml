trainer:
  logger:
    - class_path: pytorch_lightning.loggers.WandbLogger
      init_args:
        log_model: True
        tags:
          - mlp
          - risk
          - diffsort
          - case-control-sampling
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        auto_insert_metric_name: False
        monitor: hp_metric
        filename: '{epoch}-{hp_metric:.2f}'
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        patience: 50
        monitor: hp_metric
        min_delta: 0.002
        mode: max
        verbose: True
        check_finite: False
  max_epochs: 2
  check_val_every_n_epoch: 1
  val_check_interval: 0.2
  log_every_n_steps: 1
  enable_checkpointing: True
  num_sanity_val_steps: 0
  accumulate_grad_batches: 1
data:
  wandb_artifact: anon/anon/metabric.pt:latest
  batch_size: 128
  risk_set_size: 32
  num_workers: 1
model:
  head_layers: 2
  embedding_dim: 5
  head_hidden_dim: 128
  lr: 0.001
  only_covs: True
  hidden_dropout_prob: 0.1
  sorting_network: bitonic
  steepness: 19.0
  art_lambda: 0.0
  distribution: cauchy
  ignore_censoring: true
